#!/usr/bin/env python3
"""
KOMPLETN√ç ETF AUTOMATIZACE
- Spust√≠ scraping
- Automaticky p≈ôiprav√≠ data pro upload
- Integruje s web rozhran√≠m
- Scheduling a monitoring
"""

import subprocess
import os
import time
import shutil
from datetime import datetime
from pathlib import Path
import logging
import argparse
import sys

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'automation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ETFCompleteAutomation:
    def __init__(self):
        self.scraper_script = "final_scraper.py"
        self.isin_file = "ISIN.csv" 
        self.results_dir = "justetf_complete_production/results"
        self.ready_for_upload_dir = "ready_for_upload"
        
        # Vytvo≈ô slo≈æku pro p≈ôipraven√© soubory
        os.makedirs(self.ready_for_upload_dir, exist_ok=True)
    
    def run_etf_scraping(self, batch_size: int = 50, resume: bool = True) -> bool:
        """Spust√≠ kompletn√≠ ETF scraping"""
        try:
            logger.info("üöÄ SPOU≈†T√çM ETF SCRAPING")
            logger.info("="*50)
            
            cmd = [
                "python3", self.scraper_script,
                "--csv", self.isin_file,
                "--batch-size", str(batch_size)
            ]
            
            if resume:
                cmd.append("--resume")
            
            logger.info(f"P≈ô√≠kaz: {' '.join(cmd)}")
            
            start_time = datetime.now()
            
            # Spust√≠ scraper s real-time outputem
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # Zobraz√≠ output v real-time
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    logger.info(f"SCRAPER: {output.strip()}")
            
            # Poƒçkej na dokonƒçen√≠
            return_code = process.poll()
            end_time = datetime.now()
            duration = end_time - start_time
            
            if return_code == 0:
                logger.info("="*50)
                logger.info("‚úÖ SCRAPING √öSPƒö≈†Nƒö DOKONƒåEN")
                logger.info(f"‚è±Ô∏è  Doba trv√°n√≠: {duration}")
                logger.info("="*50)
                return True
            else:
                stderr = process.stderr.read()
                logger.error(f"‚ùå Scraping selhal s k√≥dem: {return_code}")
                logger.error(f"STDERR: {stderr}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi spou≈°tƒõn√≠ scrapingu: {e}")
            return False
    
    def prepare_upload_files(self) -> str:
        """P≈ôiprav√≠ nejnovƒõj≈°√≠ CSV soubor pro upload"""
        try:
            logger.info("üìÅ P≈ôipravuji soubory pro upload...")
            
            results_path = Path(self.results_dir)
            if not results_path.exists():
                logger.error(f"‚ùå Slo≈æka s v√Ωsledky neexistuje: {self.results_dir}")
                return None
            
            # Najdi nejnovƒõj≈°√≠ CSV soubor
            csv_files = list(results_path.glob("*.csv"))
            if not csv_files:
                logger.error("‚ùå ≈Ω√°dn√© CSV soubory nalezeny")
                return None
            
            latest_csv = max(csv_files, key=lambda f: f.stat().st_mtime)
            logger.info(f"üìÑ Nejnovƒõj≈°√≠ CSV: {latest_csv}")
            
            # Zkop√≠ruj do upload slo≈æky s jednoduch√Ωm n√°zvem
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            upload_filename = f"etf_data_{timestamp}.csv"
            upload_path = os.path.join(self.ready_for_upload_dir, upload_filename)
            
            shutil.copy2(latest_csv, upload_path)
            logger.info(f"‚úÖ Soubor p≈ôipraven pro upload: {upload_path}")
            
            return upload_path
            
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi p≈ô√≠pravƒõ upload soubor≈Ø: {e}")
            return None
    
    def show_upload_instructions(self, csv_file: str):
        """Zobraz√≠ instrukce pro manu√°ln√≠ upload"""
        logger.info("="*60)
        logger.info("üì§ INSTRUKCE PRO UPLOAD DAT")
        logger.info("="*60)
        logger.info("1. Otev≈ôete webov√Ω prohl√≠≈æeƒç")
        logger.info("2. P≈ôejdƒõte na: http://localhost:8083/admin?password=Omitac116")
        logger.info(f"3. Nahrajte soubor: {csv_file}")
        logger.info("4. Poƒçkejte na zpracov√°n√≠ a √∫spƒõ≈°n√© nahr√°n√≠")
        logger.info("="*60)
        
        # Otev≈ôe slo≈æku s p≈ôipraven√Ωm souborem
        try:
            if sys.platform.startswith('darwin'):  # macOS
                subprocess.run(['open', os.path.dirname(csv_file)])
            elif sys.platform.startswith('win'):   # Windows
                subprocess.run(['explorer', os.path.dirname(csv_file)])
            elif sys.platform.startswith('linux'): # Linux
                subprocess.run(['xdg-open', os.path.dirname(csv_file)])
        except:
            pass
    
    def run_complete_automation(self, batch_size: int = 50, resume: bool = True, 
                              manual_upload: bool = True) -> bool:
        """Spust√≠ kompletn√≠ automatizaci"""
        
        logger.info("="*60)
        logger.info("ü§ñ ETF KOMPLETN√ç AUTOMATIZACE - START")
        logger.info("="*60)
        
        total_start_time = datetime.now()
        
        try:
            # KROK 1: Scraping
            if not self.run_etf_scraping(batch_size=batch_size, resume=resume):
                logger.error("‚ùå Automatizace selhala na kroku: SCRAPING")
                return False
            
            # KROK 2: P≈ô√≠prava soubor≈Ø
            csv_file = self.prepare_upload_files()
            if not csv_file:
                logger.error("‚ùå Automatizace selhala na kroku: P≈ò√çPRAVA SOUBOR≈Æ")
                return False
            
            # KROK 3: Upload instrukce (nebo automatick√Ω upload)
            if manual_upload:
                self.show_upload_instructions(csv_file)
            else:
                # TODO: Implementovat automatick√Ω upload p≈ôes API
                logger.info("‚ö†Ô∏è  Automatick√Ω upload nen√≠ zat√≠m implementov√°n")
                self.show_upload_instructions(csv_file)
            
            total_end_time = datetime.now()
            total_duration = total_end_time - total_start_time
            
            logger.info("="*60)
            logger.info("‚úÖ AUTOMATIZACE √öSPƒö≈†Nƒö DOKONƒåENA")
            logger.info(f"‚è±Ô∏è  Celkov√Ω ƒças: {total_duration}")
            logger.info(f"üìÑ P≈ôipraven√Ω soubor: {csv_file}")
            logger.info("="*60)
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Neoƒçek√°van√° chyba v automatizaci: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description='ETF Complete Automation')
    parser.add_argument('--batch-size', type=int, default=50, help='Velikost batch pro scraping')
    parser.add_argument('--no-resume', action='store_true', help='Nespou≈°tƒõt resume mode')
    parser.add_argument('--auto-upload', action='store_true', help='Pokusit se o automatick√Ω upload (experiment√°ln√≠)')
    
    args = parser.parse_args()
    
    automation = ETFCompleteAutomation()
    
    success = automation.run_complete_automation(
        batch_size=args.batch_size,
        resume=not args.no_resume,
        manual_upload=not args.auto_upload
    )
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()